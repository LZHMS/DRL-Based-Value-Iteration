## Deep Reinforcement Learning Based on Value Iteration
### Theories
$$
V^{\pi}(s)=E_{\tau\sim p(\tau_t)}[\sum_{i=0}^\infty \gamma^ir_{t+i}|s_t=s]
$$
For a specific state $s$ï¼Œthe agent needs to choose the optimal action with the highest value.

### Experiments
Now we want to check how the discount factor influences the value function from the same start state.

<div class="justified-gallery">
<img src="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/randomPolicy.pnt0kxzusv4.gif" alt="randomPolicy" />
<img src="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/optimalPolicy.2wskea2qtzi0.gif" alt="optimalPolicy" />
</div>

### Results
<img src="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/Rewards.2uwj07wcwru0.webp" alt="Rewards" />

|Discount Factor|Random Cum_Reward|Random_Aver_Reward|Optimal Cum_Reward|Optimal_Aver_Reward|
|:-----:|:-----:|:-----:|:-----:|:-----:|
|0.00|-37|-3.70|-20|-2.00|
|0.05|-10|-1.00|-20|-1.00|
|0.10|-55|-5.50|10|0.91|
|0.15|-37|-3.70|11|1.10|
|0.20|-55|-5.50|-20|-1.00|
|0.25|-28|-2.80|15|2.50|
|0.30|-46|-4.60|11|1.10|
|0.35|-28|-2.80|5|0.31|
|0.40|-10|-1.00|7|0.50|
|0.45|-37|-3.70|7|0.50|
|0.50|-64|-6.40|7|0.50|
|0.55|-19|-1.90|13|1.60|
|0.60|-28|-2.80|9|0.75|
|0.65|-46|-4.60|10|0.91|
|0.70|-37|-3.70|9|0.75|
|0.75|-46|-4.60|6|0.40|
|0.80|-37|-3.70|4|0.24|
|0.85|-37|-3.70|7|0.50|
|0.90|-28|-2.80|7|0.50|
|0.95|-37|-3.70|5|0.31|
|1.00|-37|-3.70|11|1.10|

### Conclusions

### Contributors
+ Zhihao Li
### References

